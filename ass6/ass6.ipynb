{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b78dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9f9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('Iris.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('Species', axis=1)  # Features\n",
    "y = df['Species']                 # Target class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16db1ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train Na√Øve Bayes classifier\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647e4204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Show predictions (optional)\n",
    "print(\"Predicted Labels:\\n\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a8b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fb1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "error_rate = 1 - accuracy\n",
    "precision = precision_score(y_test, y_pred, average='macro')  # average macro for multiclass\n",
    "recall = recall_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899bf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy      : {accuracy:.2f}\")\n",
    "print(f\"Error Rate    : {error_rate:.2f}\")\n",
    "print(f\"Precision     : {precision:.2f}\")\n",
    "print(f\"Recall        : {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aca7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TP, FP, FN, TN for each class\n",
    "for i, label in enumerate(model.classes_):\n",
    "    TP = cm[i, i]\n",
    "    FP = cm[:, i].sum() - TP\n",
    "    FN = cm[i, :].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "    \n",
    "    print(f\"\\nClass: {label}\")\n",
    "    print(f\"True Positives (TP): {TP}\")\n",
    "    print(f\"False Positives (FP): {FP}\")\n",
    "    print(f\"False Negatives (FN): {FN}\")\n",
    "    print(f\"True Negatives (TN): {TN}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow_env)",
   "language": "python",
   "name": "tensorflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
